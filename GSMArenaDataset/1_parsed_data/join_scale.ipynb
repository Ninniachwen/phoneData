{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'brand', 'model', 'date', 'year', 'display_size_inches',\n",
      "       'internal_memory_in_gb', 'primary_camera_mega_pixel', 'primary_camera',\n",
      "       'loud_speaker', 'gps', 'colors', 'approx_price_eur', 'battery_mah',\n",
      "       'ram_in_gb'],\n",
      "      dtype='object')\n",
      "Index(['id', 'brand', 'model', 'date', 'year', 'display_size_inches',\n",
      "       'internal_memory_in_gb', 'primary_camera_mega_pixel', 'primary_camera',\n",
      "       'loud_speaker', 'gps', 'colors', 'approx_price_eur', 'battery_mah',\n",
      "       'ram_in_gb'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames (outer join to retain all rows)\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"2017_GSM_parsed.csv\", delimiter=\",\", header=\"infer\")\n",
    "print(df1.columns)\n",
    "df2 = pd.read_csv(\"2024_GSM_parsed.csv\", delimiter=\",\", header=\"infer\")\n",
    "print(df2.columns)\n",
    "\n",
    "merged_df = pd.concat([df1, df2])\n",
    "\n",
    "# Initialize a final DataFrame to store the cleaned rows\n",
    "final_rows = []\n",
    "\n",
    "# Group by the 'id' column to compare duplicates\n",
    "for id, group in merged_df.groupby('id'):\n",
    "    if len(group) == 1:\n",
    "        # Case 1: Unique or Identical Row (only one row in group)\n",
    "        final_rows.append(group.iloc[0])\n",
    "    else:\n",
    "        # Case 2: Multiple Rows (Duplicates found)\n",
    "        combined_row = group.iloc[0].copy()  # Start with the first row\n",
    "        conflicting_rows = []\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            for col in row.index:\n",
    "                if pd.isna(combined_row[col]) and not pd.isna(row[col]):\n",
    "                    # Fill missing values\n",
    "                    combined_row[col] = row[col]\n",
    "                elif combined_row[col] != row[col] and not pd.isna(combined_row[col]) and not pd.isna(row[col]):\n",
    "                    # Conflict detected\n",
    "                    conflicting_rows.append(row)\n",
    "\n",
    "        # If no conflicts, keep the combined row\n",
    "        if not conflicting_rows:\n",
    "            final_rows.append(combined_row)\n",
    "        else:\n",
    "            # Keep all conflicting rows with unique ids\n",
    "            counter = 1\n",
    "            combined_row['id'] = f\"{id} ({counter})\"\n",
    "            final_rows.append(combined_row)\n",
    "            \n",
    "            print(conflicting_rows)\n",
    "\n",
    "            for conflict_row in conflicting_rows:\n",
    "                counter += 1\n",
    "                conflict_row = conflict_row.copy()\n",
    "                conflict_row['id'] = f\"{id} ({counter})\"\n",
    "                final_rows.append(conflict_row)\n",
    "\n",
    "# Combine all rows back into a DataFrame\n",
    "merged_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Drop intermediate '_df1' and '_df2' suffixes, if created during merge\n",
    "merged_df.columns = [col.split('_')[0] for col in merged_df.columns]\n",
    "\n",
    "# Display final DataFrame\n",
    "print(merged_df)\n",
    "#duplicates = {'2g_bands':'2g', '3g_bands':'3g', '4g_bands':'4g', 'network_speed', 'gprs', 'edge', 'announced', 'status', 'dimentions':'dimensions', 'sim', 'display_type', 'display_resolution', 'display_size', 'os', 'cpu', 'chipset', 'gpu', 'memory_card':'memory(external)', 'internal_memory':'memory(internal)', 'ram', 'primary_camera', 'secondary_camera', 'loud_speaker', 'audio_jack', 'wlan', 'bluetooth', 'gps', 'nfc', 'radio', 'usb', 'sensors', 'battery', 'colors', 'approx_price_eur', 'img_url'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data saved to ../2_joined_data/2017_GSM_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "file_path = '2017_GSM_parsed.csv'\n",
    "scaled_file_path = '../2_joined_data/2017_GSM_scaled.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Apply StandardScaler to the numerical columns and add them as new columns\n",
    "scaler = StandardScaler()\n",
    "scaled_cols = [col + '_scale' for col in numerical_cols]\n",
    "scaled_data = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Create a new DataFrame with scaled columns\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=scaled_cols, index=df.index)\n",
    "\n",
    "# Combine original and scaled columns\n",
    "for orig_col, scaled_col in zip(numerical_cols, scaled_cols):\n",
    "    # Find the position of the original column\n",
    "    col_idx = df.columns.get_loc(orig_col)\n",
    "    # Insert the scaled column right after the original column\n",
    "    df.insert(col_idx + 1, scaled_col, scaled_df[scaled_col])\n",
    "\n",
    "# Step 4 (Optional): Save the scaled data back to a new CSV file\n",
    "df.to_csv(scaled_file_path, index=False)\n",
    "\n",
    "print(f\"Scaled data saved to {scaled_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "C:\\Users\\meike\\AppData\\Local\\Temp\\ipykernel_26332\\2300377711.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(group_col).apply(scale_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped scaled data saved to ../2_joined_data/2017_GSM_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "file_path = '2017_GSM_parsed.csv'\n",
    "scaled_file_path = '../2_joined_data/2017_GSM_scaled.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = numerical_cols[numerical_cols != 'year']\n",
    "\n",
    "# Apply StandardScaler within each group\n",
    "group_col = 'year'\n",
    "def scale_group(group):\n",
    "    scaler = StandardScaler()\n",
    "    group[numerical_cols] = scaler.fit_transform(group[numerical_cols])\n",
    "    return group\n",
    "\n",
    "# Step 5: Apply scaling within each group\n",
    "df = df.groupby(group_col).apply(scale_group)\n",
    "\n",
    "df.to_csv(scaled_file_path, index=False)\n",
    "\n",
    "print(f\"Grouped scaled data saved to {scaled_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped scaled data saved to ../2_joined_data/2017_GSM_scaled.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\dev\\python\\phoneData\\.bda\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "C:\\Users\\meike\\AppData\\Local\\Temp\\ipykernel_11876\\734540216.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(group_col, group_keys=False).apply(scale_group)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# File paths\n",
    "file_path = '2017_GSM_parsed.csv'\n",
    "scaled_file_path = '../2_joined_data/2017_GSM_scaled.csv'\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify numerical columns, excluding 'year'\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = numerical_cols[numerical_cols != 'year']\n",
    "\n",
    "# Apply scaling within each group while retaining original columns\n",
    "group_col = 'year'\n",
    "\n",
    "def scale_group(group):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(group[numerical_cols])\n",
    "    scaled_cols = [col + '_scale' for col in numerical_cols]\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=scaled_cols, index=group.index)\n",
    "    for orig_col, scaled_col in zip(numerical_cols, scaled_cols):\n",
    "        # Insert scaled columns right after the original columns\n",
    "        col_idx = group.columns.get_loc(orig_col)\n",
    "        group.insert(col_idx + 1, scaled_col, scaled_df[scaled_col])\n",
    "    return group\n",
    "\n",
    "# Apply scaling to each group by the specified column (e.g., 'year')\n",
    "df = df.groupby(group_col, group_keys=False).apply(scale_group)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df.to_csv(scaled_file_path, index=False)\n",
    "\n",
    "print(f\"Grouped scaled data saved to {scaled_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
